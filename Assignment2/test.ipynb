{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from NeuralNetwork.dense import Dense\n",
    "from NeuralNetwork.activations import Sigmoid,Softmax\n",
    "from NeuralNetwork.losses import mse, mse_derive\n",
    "from NeuralNetwork.network import predict\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "ALPHA = [0.002,0.008,0.03,0.08,0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(X,Y,k,set_fold):\n",
    "    batch = int(len(X)/k)\n",
    "    start = int(set_fold*batch)\n",
    "    end = int((set_fold+1)*batch)\n",
    "    x_val = X[start:end]\n",
    "    y_val = Y[start:end]\n",
    "    x_train = np.concatenate((X[:start],X[end:]))\n",
    "    y_train = np.concatenate((Y[:start],Y[end:]))\n",
    "    return x_train,y_train,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(NN, loss, loss_derive, X, Y,epoch, alpha,k = 10, verbose = True):\n",
    "    error_TS = []\n",
    "    acc_TS = []\n",
    "    val_error_TS = []\n",
    "    val_acc_TS = []\n",
    "    for e in range(epoch):\n",
    "        errors= 0\n",
    "        acc =0\n",
    "        val_error = 0\n",
    "        val_acc = 0\n",
    "        set_fold = e%k\n",
    "        x_train,y_train,x_val,y_val = k_fold(X,Y,k,set_fold)\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            # forward\n",
    "            out = predict(NN, x)\n",
    "            if np.argmax(y)==np.argmax(out):\n",
    "                acc+=1\n",
    "            # error\n",
    "\n",
    "            errors+= loss(y, out)\n",
    "\n",
    "            # backward\n",
    "            gradient = loss_derive(y, out)\n",
    "            for layer in reversed(NN):\n",
    "                gradient = layer.backward(gradient, alpha)\n",
    "\n",
    "        for x, y in zip(x_val, y_val):\n",
    "            output = predict(NN, x)\n",
    "            if np.argmax(y)==np.argmax(output):\n",
    "                val_acc+=1\n",
    "            val_error += loss(y, output)\n",
    "        \n",
    "        errors/= len(x_train)\n",
    "        acc /=len(x_train)\n",
    "        val_error /= len(x_val)\n",
    "        val_acc /=len(x_val)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{e + 1}/{epoch}, error={round(errors,4)}, accuracy={round(acc,4)}, val_error={round(val_error,4)}, val_accuracy={round(val_acc,4)}\")\n",
    "        error_TS.append(errors)\n",
    "        acc_TS.append(acc)\n",
    "        val_error_TS.append(val_error)\n",
    "        val_acc_TS.append(val_acc)\n",
    "    return [error_TS,acc_TS,val_error_TS,val_acc_TS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(NN, loss,x_test,y_test):\n",
    "    test_error = 0\n",
    "    test_acc = 0\n",
    "    for x, y in zip(x_test, y_test):\n",
    "        output = predict(NN, x)\n",
    "        if np.argmax(y)==np.argmax(output):\n",
    "            test_acc+=1\n",
    "        test_error += loss(y, output)\n",
    "    return test_error/len(x_test),test_acc/len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./dataset/MNIST.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test,X_test =  df_train.iloc[:2000, :1], df_train.iloc[:2000, 1:]\n",
    "Y_train,X_train = df_train.iloc[2000:, :1], df_train.iloc[2000:, 1:]\n",
    "X_in = np.reshape(X_train,X_train.shape +(1,))\n",
    "Y_in = np.squeeze(np.eye(10)[Y_train])\n",
    "Y_in = np.reshape(Y_in,Y_in.shape +(1,))\n",
    "X_test = np.reshape(X_test,X_test.shape +(1,))\n",
    "Y_test = np.squeeze(np.eye(10)[Y_test])\n",
    "Y_test = np.reshape(Y_test,Y_test.shape +(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "export = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\GITHUB\\MERE_WALE\\ELL784\\Assignment2\\NeuralNetwork\\activations.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  sig = 1 + np.exp(-z)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1, error=0.131, accuracy=0.1054, val_error=0.1256, val_accuracy=0.1192\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\GITHUB\\MERE_WALE\\ELL784\\Assignment2\\test.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m alpha \u001b[39min\u001b[39;00m ALPHA:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     NN \u001b[39m=\u001b[39m [Dense(\u001b[39m784\u001b[39m,\u001b[39m200\u001b[39m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         Sigmoid(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         Dense(\u001b[39m200\u001b[39m,\u001b[39m40\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         Softmax()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         ]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     metrics \u001b[39m=\u001b[39m train(NN, mse, mse_derive, X\u001b[39m=\u001b[39;49mX_in,Y\u001b[39m=\u001b[39;49mY_in, epoch\u001b[39m=\u001b[39;49mEPOCHS, alpha\u001b[39m=\u001b[39;49malpha,k\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     export[\u001b[39mstr\u001b[39m(alpha)] \u001b[39m=\u001b[39m metrics\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     metrics_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mTraining Loss\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mTraining Accuracy\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mTest Loss\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mTest Accuracy\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;32me:\\GITHUB\\MERE_WALE\\ELL784\\Assignment2\\test.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     gradient \u001b[39m=\u001b[39m loss_derive(y, out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(NN):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         gradient \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward(gradient, alpha)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(x_val, y_val):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/GITHUB/MERE_WALE/ELL784/Assignment2/test.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     output \u001b[39m=\u001b[39m predict(NN, x)\n",
      "File \u001b[1;32me:\\GITHUB\\MERE_WALE\\ELL784\\Assignment2\\NeuralNetwork\\dense.py:14\u001b[0m, in \u001b[0;36mDense.backward\u001b[1;34m(self, output_gradient, learning_rate)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, output_gradient, learning_rate):\n\u001b[0;32m     13\u001b[0m     weights_gradient \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(output_gradient, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput\u001b[39m.\u001b[39mT)\n\u001b[1;32m---> 14\u001b[0m     input_gradient \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights\u001b[39m.\u001b[39;49mT, output_gradient)\n\u001b[0;32m     15\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m weights_gradient\n\u001b[0;32m     16\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m output_gradient\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for alpha in ALPHA:\n",
    "    NN = [Dense(784,200),\n",
    "        Sigmoid(),\n",
    "        Dense(200,40),\n",
    "        Sigmoid(),\n",
    "        Dense(40,10),\n",
    "        Softmax()\n",
    "        ]\n",
    "    metrics = train(NN, mse, mse_derive, X=X_in,Y=Y_in, epoch=EPOCHS, alpha=alpha,k=10)\n",
    "    export[str(alpha)] = metrics\n",
    "    metrics_names = ['Training Loss','Training Accuracy','Test Loss','Test Accuracy']\n",
    "    for i in range(len(metrics_names)):\n",
    "        plt.plot(metrics[i])\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(metrics_names[i])\n",
    "        plt.savefig(\"./graph/MNIST_{}.png\".format(metrics_names[i]))\n",
    "        plt.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04300479344668219"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.dumps(export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error, test_acc = testModel(NN,mse,X_test,Y_test)\n",
    "print(test_acc,test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ELL784-XmMyOxYq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
